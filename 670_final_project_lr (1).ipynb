{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2677f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "torch.manual_seed(1)\n",
    "all_data=pd.read_csv('WikiLarge_Train.csv')\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss= StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in sss.split(all_data, all_data['label']):\n",
    "    train_df=all_data.iloc[train_index]\n",
    "    dev_df=all_data.iloc[test_index]\n",
    "train_df['label'].value_counts()/len(train_df)\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba8626d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401297</th>\n",
       "      <td>Even when their burrow is underwater , mudskip...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347368</th>\n",
       "      <td>The first documented mention of Hungen is in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325250</th>\n",
       "      <td>1921 - Karel Appel , Dutch painter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3693</th>\n",
       "      <td>Both GQ and In Style had Alba on their June 20...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246480</th>\n",
       "      <td>Design The most well-known type of electrode i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            original_text  label\n",
       "401297  Even when their burrow is underwater , mudskip...      0\n",
       "347368  The first documented mention of Hungen is in t...      0\n",
       "325250                 1921 - Karel Appel , Dutch painter      0\n",
       "3693    Both GQ and In Style had Alba on their June 20...      1\n",
       "246480  Design The most well-known type of electrode i...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4254ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train_df.original_text).todense()\n",
    "X_dev  = vectorizer.transform(dev_df.original_text).todense()\n",
    "\n",
    "y_train = list([int(y) for y in train_df['label']])\n",
    "y_dev  = list([int(y) for y in dev_df['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98cab5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensors(instance, label):\n",
    "    '''Given some vectorized text and the integer label for its class, returns a tuple of these in Tensor format'''\n",
    "    instance = torch.Tensor(instance)\n",
    "    label = torch.LongTensor([int(label)])\n",
    "    return (instance, label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5115f7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [to_tensors(row, y_train[i]) for i, row in enumerate(X_train)]\n",
    "dev_data = [to_tensors(row, y_dev[i]) for i, row in enumerate(X_dev)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2db9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    '''A simple multi-class logistic regression network'''\n",
    "    \n",
    "   \n",
    "    def __init__(self, vocab_size, num_classes=2):\n",
    "\n",
    "       \n",
    "        super(LogisticRegression, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.betas = nn.Linear(vocab_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, feature_vector):\n",
    "        '''Takes in the input to the network and then passes it through this\n",
    "        network's layers(s) and returns the output of the final layer.'''\n",
    "\n",
    "        \n",
    "        return self.betas(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18824edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8523251afb834a87b6c5b7fc231f50cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=5.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training Loop for Epoch 0', max=333414.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training Loop for Epoch 1', max=333414.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training Loop for Epoch 2', max=333414.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training Loop for Epoch 3', max=333414.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Training Loop for Epoch 4', max=333414.0, style=ProgressS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "vocab_size = len(vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "model = LogisticRegression(vocab_size)\n",
    "\n",
    "# We'll use a low learning rate\n",
    "learning_rate = 5e-5\n",
    "\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n",
    "\n",
    "\n",
    "for epoch in trange(5, desc='Epoch'):\n",
    "    \n",
    " \n",
    "    for (instance, label) in tqdm(training_data, leave=False, desc='Training Loop for Epoch %d' % epoch):\n",
    "\n",
    "       \n",
    "        model.zero_grad()        \n",
    "                \n",
    "    \n",
    "        output = model(instance)\n",
    "        \n",
    "       \n",
    "        loss = loss_function(output, label)\n",
    "        \n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8027200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep track of our predictions for each instance\n",
    "dev_preds = []\n",
    "\n",
    "# Normally, every computation in PyTorch will keep around some state\n",
    "# information which will let us compute the gradients with the loss\n",
    "# function. However, if we're doing prediction, we *don't* want that, \n",
    "# as it is expensive to keep and compute, and can leave around extra\n",
    "# state information that we might forget.\n",
    "#\n",
    "# To avoid keeping around the state information, we can use \n",
    "# torch.no_grad() in a with statement, or we can call model.eval()\n",
    "# to tell the model to switch into prediction-only model. We can\n",
    "# switch back into training mode with model.train() too.\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Let's make a prediction for each item in the dev set\n",
    "    for (instance, label) in dev_data:\n",
    "        \n",
    "        # Note that these outputs are the unnormalized activations\n",
    "        # of the last layer--not probabilities. However, we don't\n",
    "        # actually need the probabilities to make predictions!\n",
    "        outputs = model(instance)\n",
    "        \n",
    "        # To make a prediction, we can simply look for which neuron (index)\n",
    "        # had the highest activation. If we ran these through softmax,\n",
    "        # that neuron would still be more probable, but this saves us\n",
    "        # the computation time. \n",
    "        #\n",
    "        # To find the index with the highest value, we'll use np.argmax,\n",
    "        # which searches and returns that index.\n",
    "        dev_preds.append(np.argmax(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e767435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def result_evalation(predict_result, true_result):\n",
    "    print(\"accuracy:\", accuracy_score(true_result, predict_result))\n",
    "    print(\"precision:\", precision_score(true_result, predict_result))\n",
    "    print(\"recall:\", recall_score(true_result, predict_result))\n",
    "    print(\"f1:\", f1_score(true_result, predict_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5790792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6553614703553519\n",
      "precision: 0.6733508245877061\n",
      "recall: 0.6034743383640857\n",
      "f1: 0.6365005251236888\n"
     ]
    }
   ],
   "source": [
    "result_evalation(dev_preds,y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee7b51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('WikiLarge_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69f57ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = vectorizer.transform(test_data.original_text).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42f302bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [to_tensors(row, 0) for i, row in enumerate(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1b046b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll keep track of our predictions for each instance\n",
    "test_preds = []\n",
    "\n",
    "# Normally, every computation in PyTorch will keep around some state\n",
    "# information which will let us compute the gradients with the loss\n",
    "# function. However, if we're doing prediction, we *don't* want that, \n",
    "# as it is expensive to keep and compute, and can leave around extra\n",
    "# state information that we might forget.\n",
    "#\n",
    "# To avoid keeping around the state information, we can use \n",
    "# torch.no_grad() in a with statement, or we can call model.eval()\n",
    "# to tell the model to switch into prediction-only model. We can\n",
    "# switch back into training mode with model.train() too.\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Let's make a prediction for each item in the dev set\n",
    "    for (instance, label) in test_data:\n",
    "        \n",
    "        # Note that these outputs are the unnormalized activations\n",
    "        # of the last layer--not probabilities. However, we don't\n",
    "        # actually need the probabilities to make predictions!\n",
    "        outputs = model(instance)\n",
    "        \n",
    "        # To make a prediction, we can simply look for which neuron (index)\n",
    "        # had the highest activation. If we ran these through softmax,\n",
    "        # that neuron would still be more probable, but this saves us\n",
    "        # the computation time. \n",
    "        #\n",
    "        # To find the index with the highest value, we'll use np.argmax,\n",
    "        # which searches and returns that index.\n",
    "        test_preds.append(np.argmax(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36531f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(1),\n",
       " tensor(1),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7bc2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_new=[]\n",
    "for i in test_preds:\n",
    "    test_preds_new.append(int(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f90da34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53486"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_preds_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4428b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame({'id':list(test_data['id']), 'label':test_preds_new})\n",
    "result.to_csv('results_LR.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01280935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53486"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1cbc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
